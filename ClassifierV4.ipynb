{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6cfb988",
   "metadata": {},
   "source": [
    "# Document Classifier V4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9044b0bb",
   "metadata": {},
   "source": [
    "### This script iterates through files inside a specified folder and outputs the filename, 5 tags to describe the file, and an advanced summary of the file contents using the Huggingface Transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4781a015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5101 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Your max_length is set to 1024, but you input_length is only 871. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=435)\n",
      "Your max_length is set to 1024, but you input_length is only 908. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=454)\n",
      "Your max_length is set to 1024, but you input_length is only 899. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=449)\n",
      "Your max_length is set to 1024, but you input_length is only 793. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=396)\n",
      "Your max_length is set to 1024, but you input_length is only 840. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=420)\n",
      "Your max_length is set to 1024, but you input_length is only 804. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=402)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Financial-Analyst-Course-Curriculum.pdf\n",
      "Tags: ['financial', 'excel', 'analysis', 'accounting', 'lecture']\n",
      "Summary: Excel is the world's #1 software for the Office. Learn about the Excel interface – How to perform data entry in Excel. Use Excel's Freeze Panes to Handle Large Datasets. Use the “Tell me what you want to do’ function to find Excel functionalities. Learn how to scroll fast and be much quicker in Excel. Using Named Ranges to Make Formulas More Readable. Using Customsort to Sort Multiple Columns Within a Table. Using Lookup Functions ( Vlookup ) to Fill the Database Sheet. Excel Charts - The Easy Way to Do It – Learn how to create charts in Excel 2016 65. Make Your Excel Charts Look Sexier -Proven Tips --> Intermediate Excel 66. Creating a Bridge Chart in Excel2016 -As Easy as It Gets 67. New Ways to Visualize Your Data -TreemapCharts 68. How to Represent Trends with Sparklines 10 Financial Statement Excel Financial Modeling Accounting Financial Analysis Finance 101 Capital Budgeting PowerPoint Analysis Excel – Pivot Tables & Slicers Lecture Lecture Title Degree of difficulty : 77. Introduction to the Course Challenge Course Challenge 1. Accounting: The Science Behind Financial Figures. The Three Core Financial Statements : P&L , Balance Sheet and Cash Flow. How to Calculate Depreciation Expense any Accounting system. Case Study -The Accounting Transactions of a Lemonade Stand (1/4) Timing of Income : Principles of Revenue Recognition. Timing of Expenses: Principles of Cost Recognition – When to recognize revenue – Whentorecognize costs. The Four Type of Accruals Arising in Accounting Transactions – How to work with accruals --> Intermediate Accounting. Calculating the present value of future cash flows – Calculatenetpresentvalue -->Finance and financial math basics 28 Financial Statement Excel Financial Modeling Accounting Financial Analysis Finance 101 Capital Budgeting PowerPoint Analysis finance 101 – Loan Calculations Lecture Lecture Title Degree of difficulty : 158. Using Annuities 159. Calculating a Complete Loan Schedule What you will learn : – Use annuities Course Challenge 3 – Calculateacompleteloanschedule.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but you input_length is only 815. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=407)\n",
      "Your max_length is set to 1024, but you input_length is only 824. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=412)\n",
      "Your max_length is set to 1024, but you input_length is only 829. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=414)\n",
      "Your max_length is set to 1024, but you input_length is only 826. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=413)\n",
      "Your max_length is set to 1024, but you input_length is only 722. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=361)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: test.pdf\n",
      "Tags: ['data', 'project', 'nasa', 'stock', 'consumable']\n",
      "Summary: The primary objective of this data science project is to overalloperationaleffectivenessofthesecriticaltestingfacili- conduct a comprehensive analysis of NASA’s consumable ties. Through datadriven in- sights and predictive analytics , we aim to contribute to the optimization of resource allocation. ConsumablesDataScienceProjectProposal—2/4 science to optimize consumable management. oxygen, helium , and other relevant gases will be a central sourceofinformationforthisproject. Historical pricing data for consumables will be sourced withinNASA’swindtunnelfacilities. Financial reports may requireparsingtoextractrelevantfinan- couldimpacttheavailabilityandpricingofconsumables. onsumablesuppliers. Byassessing marketdatawillundergorigorousdatapreprocessing, encom- thefinancialhealthandmarketperformanceofsuppliers, we passing tasks such as cleaning , validation , and structuring. Time series markettrends will be used to assess the financial health and stability of these fluctuations. The acquireddatawillundergothoroughcleaningtoaddress issues such as missingvalues, outliers, and dataformatstandardiza. cessmayinvolvewebscrapingusing tools likePython’s BeautifulSoupandSelenium. The successful execution of this Deliverable : Initiateddatacollectionprocess . project offers several significant benefits to NASA. By achieving cost savings and resource op- NASA can redirect funds toward critical areas forconsumablesuppliers such as facility upgrades, staff development, and research.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import docx\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from transformers import pipeline, BartTokenizer\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Additional cleaning to handle special cases in document text extraction.\"\"\"\n",
    "    text = re.sub(r'([a-zA-Z])-([a-zA-Z])', r'\\1\\2', text)\n",
    "    text = re.sub(r'([a-zA-Z])([,.:;!?\\)])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'([,.:;!?\\(])([a-zA-Z])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def extract_text(file_path):\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    if file_extension.lower() == '.pdf':\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_extension.lower() in ['.doc', '.docx']:\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_extension.lower() == '.txt':\n",
    "        return extract_text_from_txt(file_path)\n",
    "    elif file_extension.lower() == '.csv':\n",
    "        return extract_text_from_csv(file_path)\n",
    "    elif file_extension.lower() == '.json':\n",
    "        return extract_text_from_json(file_path)\n",
    "    return \"\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        full_text = \" \".join(page.extract_text() or '' for page in pdf.pages)\n",
    "    return clean_text(full_text)\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = docx.Document(docx_path)\n",
    "    full_text = \" \".join(para.text for para in doc.paragraphs if para.text.strip())\n",
    "    return clean_text(full_text)\n",
    "\n",
    "def extract_text_from_txt(txt_path):\n",
    "    with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "        full_text = file.read()\n",
    "    return clean_text(full_text)\n",
    "\n",
    "def extract_text_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    full_text = \" \".join(df.astype(str).sum(axis=1))\n",
    "    return clean_text(full_text)\n",
    "\n",
    "def extract_text_from_json(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    full_text = ' '.join([str(value) for value in data.values()])\n",
    "    return clean_text(full_text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(re.sub(r'\\W', ' ', text.lower()))\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 1]\n",
    "    return words\n",
    "\n",
    "def perform_topic_modeling(words):\n",
    "    if not words:\n",
    "        return []\n",
    "    num_topics = 1\n",
    "    num_words = 5\n",
    "    dictionary = corpora.Dictionary([words])\n",
    "    corpus = [dictionary.doc2bow(words)]\n",
    "    if not corpus:\n",
    "        return []\n",
    "    lda_model = models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    topics = lda_model.print_topics(num_words=num_words)\n",
    "    return topics\n",
    "\n",
    "def summarize_text(text, summarizer, tokenizer, max_length=1024, min_length=50):\n",
    "    \"\"\"Generates a summary of the text, handles text longer than the model's maximum length limit.\"\"\"\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokens = tokenizer.encode(cleaned_text, return_tensors='pt', add_special_tokens=True)\n",
    "    if tokens.size(1) > max_length:\n",
    "        part_length = int(len(cleaned_text) / (tokens.size(1) / max_length + 1))\n",
    "        parts = [cleaned_text[i:i + part_length] for i in range(0, len(cleaned_text), part_length)]\n",
    "        summaries = [summarizer(part, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text'] for part in parts if len(part) > min_length]\n",
    "        return ' '.join(summaries)\n",
    "    else:\n",
    "        summary = summarizer(cleaned_text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "        return summary[0]['summary_text']\n",
    "\n",
    "def correct_spelling(text):\n",
    "    \"\"\"Corrects spelling in the summarized text.\"\"\"\n",
    "    spell = SpellChecker()\n",
    "    words = text.split()\n",
    "    corrected_words = [spell.correction(word) if spell.unknown([word]) else word for word in words]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "def process_directory(directory_path, summarizer, tokenizer):\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            extracted_text = extract_text(file_path)\n",
    "            words = preprocess_text(extracted_text)\n",
    "            topics = perform_topic_modeling(words)\n",
    "            tags = [word for _, topic in topics for word in re.findall(r'\\\"(.*?)\\\"', topic) if topics]\n",
    "            summary = summarize_text(extracted_text, summarizer, tokenizer)\n",
    "            print(f\"File: {filename}\")\n",
    "            if tags:\n",
    "                print(f\"Tags: {tags}\")\n",
    "            print(f\"Summary: {summary}\\n\")\n",
    "            \n",
    "directory_path = \"testdocuments\"\n",
    "process_directory(directory_path, summarizer, tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
